\documentclass[letterpaper,superscriptaddress,showkeys,longbibliography]{revtex4-1}
\usepackage[utf8]{inputenc}
\usepackage{color,dcolumn,graphicx,hyperref}
\usepackage{natbib}
\usepackage{footnote}
\usepackage{threeparttable}
\usepackage{caption}
\usepackage{scrextend}
\usepackage{setspace}
\usepackage[margin=1in]{geometry}

% stuff for coloring rows in tables - BEGIN
\usepackage{booktabs}% http://ctan.org/pkg/booktabs
\usepackage{colortbl}% http://ctan.org/pkg/colortbl
\usepackage{amsmath}% http://ctan.org/pkg/amsmath
\usepackage{xcolor}% http://ctan.org/pkg/xcolor
\usepackage{graphicx}% http://ctan.org/pkg/graphicx

\colorlet{tableheadcolor}{gray!25} % Table header colour = 25% gray
\newcommand{\headcol}{\rowcolor{tableheadcolor}} %
\colorlet{tablerowcolor}{gray!10} % Table row separator colour = 10% gray
\newcommand{\rowcol}{\rowcolor{tablerowcolor}} %
    % Command \topline consists of a (slightly modified) \toprule followed by a \heavyrule rule of colour tableheadcolor (hence, 2 separate rules)
\newcommand{\topline}{\arrayrulecolor{black}\specialrule{0.1em}{\abovetopsep}{0pt}%
            \arrayrulecolor{tableheadcolor}\specialrule{\belowrulesep}{0pt}{0pt}%
            \arrayrulecolor{black}}
    % Command \midline consists of 3 rules (top colour tableheadcolor, middle colour black, bottom colour white)
\newcommand{\midline}{\arrayrulecolor{tableheadcolor}\specialrule{\aboverulesep}{0pt}{0pt}%
            \arrayrulecolor{black}\specialrule{\lightrulewidth}{0pt}{0pt}%
            \arrayrulecolor{white}\specialrule{\belowrulesep}{0pt}{0pt}%
            \arrayrulecolor{black}}
    % Command \rowmidlinecw consists of 3 rules (top colour tablerowcolor, middle colour black, bottom colour white)
\newcommand{\rowmidlinecw}{\arrayrulecolor{tablerowcolor}\specialrule{\aboverulesep}{0pt}{0pt}%
            \arrayrulecolor{black}\specialrule{\lightrulewidth}{0pt}{0pt}%
            \arrayrulecolor{white}\specialrule{\belowrulesep}{0pt}{0pt}%
            \arrayrulecolor{black}}
    % Command \rowmidlinewc consists of 3 rules (top colour white, middle colour black, bottom colour tablerowcolor)
\newcommand{\rowmidlinewc}{\arrayrulecolor{white}\specialrule{\aboverulesep}{0pt}{0pt}%
            \arrayrulecolor{black}\specialrule{\lightrulewidth}{0pt}{0pt}%
            \arrayrulecolor{tablerowcolor}\specialrule{\belowrulesep}{0pt}{0pt}%
            \arrayrulecolor{black}}
    % Command \rowmidlinew consists of 1 white rule
\newcommand{\rowmidlinew}{\arrayrulecolor{white}\specialrule{\aboverulesep}{0pt}{0pt}%
            \arrayrulecolor{black}}
    % Command \rowmidlinec consists of 1 tablerowcolor rule
\newcommand{\rowmidlinec}{\arrayrulecolor{tablerowcolor}\specialrule{\aboverulesep}{0pt}{0pt}%
            \arrayrulecolor{black}}
    % Command \bottomline consists of 2 rules (top colour
\newcommand{\bottomline}{\arrayrulecolor{white}\specialrule{\aboverulesep}{0pt}{0pt}%
            \arrayrulecolor{black}\specialrule{\heavyrulewidth}{0pt}{\belowbottomsep}}%
\newcommand{\bottomlinec}{\arrayrulecolor{tablerowcolor}\specialrule{\aboverulesep}{0pt}{0pt}%
            \arrayrulecolor{black}\specialrule{\heavyrulewidth}{0pt}{\belowbottomsep}}%
% stuff for coloring rows in tables - END

\newcolumntype{L}{>{\raggedright\arraybackslash}m{5cm}} % creates now column type to wrap text
\newcolumntype{B}{>{\raggedright\arraybackslash}m{3cm}} % creates now column type to wrap text
\newcolumntype{C}{>{\raggedright\arraybackslash}m{3.5cm}} % creates now column type to wrap text
\newcolumntype{D}{>{\raggedright\arraybackslash}m{2.5cm}} % creates now column type to wrap text

\hypersetup
{
    colorlinks = true, linkcolor = blue, citecolor = blue, urlcolor = blue,
}

\newcommand{\ignore}[1]{}

% use these two lines to force table positioning
% \usepackage{float} 
% \restylefloat{table}

\begin{document}

<<setup, include=FALSE>>=
  ## set global chunk options
  opts_chunk$set(fig.path='figure/', fig.align='center', out.width=".7\\linewidth", par=TRUE, warning=FALSE, message=FALSE, error=FALSE, comment=NA)
@

\setstretch{2} % double space the entire manuscript

\title{Consuming article-level metrics: observations and lessons}

\author{Scott Chamberlain}
\email[E-mail: ]{scott@ropensci.org}
\affiliation{Biology Dept., Simon Fraser University, Burnaby, BC, Canada V5B 1E1}

\keywords{altmetrics; R; sotware; API; data; provenance}

\maketitle

\section*{Introduction}

The Journal Impact Factor (JIF) \cite{garfield1955,garfield2006} is a summation of the impact of all articles in a journal based on citations (owned and published by Thomson Reuters). Publishers have used the JIF to gain recognition, and authors are now evaluated by their peers based on the JIF of the journals they have published in \cite{monastersky2005}, and authors often choose where to publish based on the JIF. The JIF has significant flaws, including being subject to gaming \cite{ploseditorial}, and not being reproducible \cite{rossner2007}. In fact, the San Francisco Declaration on Research Assessment has a growing list of scientists and societies that would like to stop the use of the JIF in judging work of scientists (see \cite{alberts2013} for commentary). An important critique of the JIF is that it doesn't measure the impact of individual articles - clearly not all articles in a journal are of the same caliber. Altmetrics measure the impact of individual articles, including usage (e.g., pageviews, downloads), citations, and social metrics (e.g., Twitter, Facebook) \cite{priem2012}. There may be a distinction where \emph{article level metrics} refer to usage and \emph{altmetrics} to social media metrics; hereafter, I use the term \emph{altmetrics} to usage, citation, and social media. Altmetrics have many advantages over the JIF, including: a) Openness: Altmetrics are largely based off of data that is open to anyone (though there are some that aren't, e.g., Web of Science, Scopus). If data sources are open, conclusions based on altmetrics can be verified by others, and tools can be built on top of altmetrics; b) Speed: Altmetrics are nearly real-time metrics of scholarly impact \cite{priem2012} - citations can take years to accrue, but mentions and discussion that can be searched on the web take hours or days; c) Diversity of sources: Altmetrics include far more than just citations, and provide metrics in a variety of domains, including discussion by the media (mentions in the news), discussion by the public (facebook likes, tweets), and importance to colleagues (citations).

There are many potential uses for altmetrics, including: 

\begin{itemize}
  \item \emph{Research}. As altmetrics rise in use and popularity, research on altmetrics themselves will inevitably become a more common use case. Some recent papers have answered the questions: How do different altmetrics relate to one another \cite{yan2011,bollen2009}? What is the role of Twitter in the lifecycle of a paper \cite{darling2013}? Can tweets predict citations \cite{eysenbach2011,thelwall2013}? These questions involve collecting altmetrics in bulk from altmetrics providers, and manipulating, visualizing, and analyzing the data. This use case often requires using scripting languages (e.g., Python, Ruby, R) to consume altmetrics. Consuming altmetrics from this perspective is somewhat different than the use case in which a user views altmetrics hosted elsewhere in the cloud. The "local" use case is the target use case on which this paper is concerned. 
  \item \emph{Credit}. Scholars already put altmetrics on their CVs. With the rise of altmetrics, this will become much more common, especially with initiatives like that of NSF that allows scholars to get credit for \emph{products}, not just papers - and products like videos or presentation can not be measured by citations or Journal Impact Factors. This use case will involve scholars with a wide variety of technical skills; and will likely be made easy with tools from ImpactStory or other providers. Piwowar and Priem discuss this use case further \cite{piwowar2013power}. 
  \item \emph{Filtering}. Scholars could not possibly find relavant papers efficiently given that there are now thousands of scholarly journals. Individual altmetrics components can be used to filter. For example, many scientists use Twitter now, and are more likely to see a paper that is tweeted often - in a way leveraging altmetrics. Altmetrics can be used to filter more directly. For example, altmetrics are now presented alongside papers, which can be used to make decisions about what papers to read and not to read (you may be drawn to a paper with a large number of tweets, for example).
\end{itemize}

% cutout: Altmetrics can be consumed in a variety of contexts: as static text, images, or graphs alongside a pdf or website, as a javascript widget in a website and more.  

In this paper I discuss altmetrics from the perspective of developing and using scripting interfaces for altmetrics. From this perspective, there are a number of considerations: where can you get altmetrics data; data consistency; data provenance; altmetrics in context; and technical barriers to use.

\section*{Altmetrics data providers}

There are a number of publishers now presenting altmetrics alongside peer-reviewed articles on their websites and on PDFs (for examples, see Wiley-Blackwell \cite{wiley}, Nature \cite{nature}, Public Library of Science \cite{plos}, Frontiers \cite{frontiers}, Biomed Central \cite{bmcentral}). Most of these publishers do not provide public facing APIs (Application Programming Interface - a way for computers to talk to one another) for altmetrics data, but instead use Altmetric or ImpactStory to provide altmetrics data on their papers - one exception is PLoS, which collects their own altmetrics, with an open API to use their altmetrics data. At the time of writing there are four major entities that aggreagate and provide altmetrics data: PLoS, ImpactStory, Altmetric, and Plum Analytics (see Table 1 for details). There are a few other smaller scale altmetrics providers, such as CitedIn (\cite{citedin}) and ScienceCard (\cite{sciencecard}), but are relatively small in scope and breadth. PLoS and ImpactStory have open APIs, while Altmetrics API limits API requests by hour and day, and by paid vs. free accounts. PLoS provides data in JSON (JavaScript Object Notation), JSONP (JSON with padding, allowing manipulation of JSON data in a browser), and XML (Extensible Metadata Language), ImpactStory provides data in JSON, Altmetric in JSON and JSONP, and Plum Analytics in JSON. PLoS provides much more granular data than the others, with daily, monthly and yearly totals; ImpactStory provides only total values; and Altmetric provides total values, plus incremental summaries of their proprietary Altmetric score. PLoS is a publisher, while the mission of the other two is to collect and provide altmetrics data. PLoS and ImpactStory are non-profit, while Altmetric and Plum Analytics are for-profit companies.

The four providers overlap in some sources of altmetrics they gather, but not all (see Appendix Table A1). The fact that there is some complementarity in sources opens the possibility that different metrics can be combined from across the different providers to get more a complete set of altmetrics. For those that are compelementary, this should be relatively easy, and we don't have to worry about data consistency. However, when they share data sources, data may not be consistent between providers for the same data source (see \emph{Data standardization and consistency} below).

One of the important aspects of altmetrics is that most of the data collected by altmetrics aggregators like ImpactStory is that they aren't creating the data themselves, but rather are collecting the data from other sources that have their own licences. Thus, data licenses for PLoS, ImpactStory, and Altmetric generally match those of the original data provider (e.g., some data providers do not let anyone to cache data). 

% Note that in discussing the four providers, we are only aware of the details of each provider that are open to the public. For example, Altmetric and PlumAnalytics provide some or all of their services to paying customers, which I don't cover here. 

\begin{table}[!ht]
    \begin{threeparttable}[b]
    \caption{Details on the four largest altmetrics providers.}\label{tab:a} % title of Table
        \begin{tabular}{|l|l|l|l|l|}
            \hline
            Variable & PLoS & ImpactStory & Altmetric & Plum Analytics \\
            \hline
            Open API? & Yes & Yes & Limited\tnote{d} & No \\
            Data format & JSON,JSONP,XML & JSON & JSON,JSONP & JSON \\
            Granularity\tnote{b} & D,M,Y & T & I & T \\
            API Authentication & API key & API key & API key & API key \\
            Business type & Publisher & Altmetrics provider & Altmetrics provider & Altmetrics provider \\
            For-profit & No & No & Yes & Yes \\
            Income based on & Page charges & Publishers/Grants & Publishers & Institutions \\
            Rate limiting & Not enforced & Not enforced\tnote{c} & 1 call/sec.\tnote{d} & Unknown \\
            Products covered & Articles & Many\tnote{e} & Articles & Many\tnote{f} \\
            Software clients & R\tnote{g} & R,Javascript\tnote{h} & R,Python\tnote{i} & Unknown \\
            \hline
        \end{tabular}
        \begin{tablenotes}
            \item[a] Payed accounts with perks
            \item[b] D: day; M: month; Y: year; T: total; I: incremental summaries
            \item[c] Note: They recommend delaying a few seconds between requests
            \item[d] Also hourly and daily limits enforced; using API key increases limits
            \item[e] articles, code, software, presentations, datasets
            \item[f] articles, code, software, presentations, datasets, books, theses, etc. (see \url{http://www.plumanalytics.com/metrics.html} for a full list)
            \item[g] \url{https://github.com/ropensci/alm}
            \item[h] R (\url{https://github.com/ropensci/rimpactstory}), Javascript (\url{https://github.com/highwire/opensource-js-ImpactStory})
            \item[i] R (\url{https://github.com/ropensci/rAltmetric}), Python (\url{https://github.com/lnielsen-cern/python-altmetric})
        \end{tablenotes}
    \end{threeparttable}
\end{table}


\section*{Consistency}

Now that there are multiple providers for altmetrics data, data consistency is an important consideration. For example, PLoS, ImpactStory , Altmetric, and PlumAnalytics collect altmetrics from some of the same data sources. Are the numbers they present to users consisten for the same paper, or are they different due to different collection dates, data sources, or methods of collection? Each of the  providers that give aggregate altmetrics can collect and present altmetrics as needed for their target audience. However, as altmetrics consumers and researchers, we should have a clear understanding of the potential pitfalls when using altmetrics data for any purpose, especially research where data quality and consistenty is essential. 

I used a set of 565 articles (using their DOIs to search by) from PLoS journals only - this way all four providers would have data on the articles. I collected metrics from each of the four providers for each of the 565 DOIs. Note that I excluded data from PlumAnalytics for Citeulike as it was not provided (but they do collect it; pers. comm. Andrea Michalek). In addition, Facebook data was exluded from PlumAnalytics results because it was unclear how to equate their data with the data from the other providers. For each DOI I calculated the maximum difference between values (i.e., providers) and plotted the distribution of these maximum difference values for seven altmetrics that were shared among the providers (Fig. 1). Figure 1 shows that, at least with respect to absolute numbers, PMC citations are very different among providers, while PLoS views (html + pdf views; relavant only to PLoS ALM, ImpactStory, and PlumAnalytics) are somewhat less variable among providers. The remaining metrics were not very different among providers, with most values at zero, or no difference at all. 

<<dataconst_prep, eval=TRUE, echo=FALSE, warning=FALSE>>=
suppressPackageStartupMessages(library(alm, quietly=TRUE))
library(rImpactStory, quietly=TRUE)
library(rAltmetric, quietly=TRUE); 
library(ggplot2, quietly=TRUE)
library(rplos, quietly=TRUE)
library(reshape2, quietly=TRUE)
library(httr, quietly=TRUE)

plum <- function(id, auth=getOption("plumkey"), sleep=1, fileout=NULL)
{
  Sys.sleep(sleep) 
  
  url = "https://api.plu.mx/a/"
  url2 <- paste0(url, id)
  args <- compact(list(auth=auth))
  out <- content(GET(url=url2, query=args))
  
  if(!is.null(out$citation)){
    cit <- ldply(out$citation, function(x) as.data.frame(x))
  } else {cit <- NULL}
  if(!is.null(out$socialMedia)){
    socmed <- ldply(out$socialMedia, function(x) as.data.frame(x))
  } else {socmed <- NULL}
  if(!is.null(out$capture)){
    cap <- ldply(out$capture, function(x) as.data.frame(x))
  } else {cap <- NULL}
  if(!is.null(out$usage)){
    use <- ldply(out$usage, function(x) as.data.frame(x))
  } else {use <- NULL}
  if(!is.null(out$mention)){
    men <- ldply(out$mention, function(x) as.data.frame(x))
  } else {men <- NULL}
  
  df <- ldply(compact(list(citation=cit, socialmedia=socmed, captures=cap, usage=use, mentions=men)))
  
  scopus=tryCatch(df[df$source %in% "Scopus", "count"])
  if(identical(scopus,numeric(0)))
    scopus <- NA
  mendeley=tryCatch(df[df$countTypeSource %in% "READER_COUNT:Mendeley", "count"])
  if(identical(mendeley,numeric(0)))
    mendeley <- NA
  twitter=tryCatch(df[df$source %in% "Twitter", "count"])
  if(identical(twitter,numeric(0))){
    twitter <- NA
  } else { twitter <- max(twitter) }
  pmc=tryCatch(df[df$countTypeSource %in% "FULL_TEXT_VIEWS:PubMedCentral", "count"]+
    df[df$countTypeSource %in% "PDF_VIEWS:PubMedCentral", "count"])
  if(identical(pmc,numeric(0)))
    pmc <- NA
  ploscounter=tryCatch(df[df$countTypeSource %in% "HTML_VIEWS:PLoS", "count"]+
    df[df$countTypeSource %in% "PDF_VIEWS:PLoS", "count"])
  if(identical(ploscounter,numeric(0)))
    ploscounter <- NA
  dfout <- data.frame(doi = out$identifier$doi, scopus=scopus, mendeley=mendeley,
                      twitter=twitter, pmc=pmc, ploscounter=ploscounter)
  
  if(is.null(fileout)){ dfout } else
    { write.table(dfout, file=fileout, append=TRUE, row.names=FALSE, col.names=FALSE, sep=",") }
}

# Define a function to parse ImpactStory results
parse_is <- function(input){
  metrics <- input$metrics
  delicious_bookmarks <- try(metrics$`delicious:bookmarks`$values[["raw"]])
  mendeley_readers <- try(metrics$`mendeley:readers`$values[["raw"]])
  plosalm_html_views <- try(metrics$`plosalm:html_views`$values[["raw"]])
  plosalm_pdf_views <- try(metrics$`plosalm:pdf_views`$values[["raw"]])
  plosalm_scopus <- try(metrics$`plosalm:scopus`$values[["raw"]])
 # pmc_citations <- try(metrics$`pubmed:pmc_citations`$values[["raw"]])
  pmc_citations_one <- try(metrics$`plosalm:pmc_full-text`$values[["raw"]])
  pmc_citations_two <- try(metrics$`plosalm:pmc_pdf`$values[["raw"]])
  if(is.null(pmc_citations_one) & is.null(pmc_citations_one)){
    pmc_citations <- NULL
  } else
  {
    pmc_citations <- pmc_citations_one + pmc_citations_two
  }
  topsy_tweets <- try(metrics$`topsy:tweets`$values[["raw"]])
  results <- list(delicious_bookmarks=delicious_bookmarks,mendeley_readers=mendeley_readers,
                  plosalm_html_views=plosalm_html_views,plosalm_pdf_views=plosalm_pdf_views,
                  plosalm_scopus=plosalm_scopus,pmc_citations=pmc_citations,topsy_tweets=topsy_tweets)
  results[sapply(results, is.null)] <- NA
  data.frame(results, date_modified=input$last_modified)
}

# Define a function to collect altmetrics from four providers
compare_altmet_prov <- function(doi, isaddifnot=FALSE, fileout=NULL, sleep=0){
  Sys.sleep(sleep)
  
  # plos
  plos_dat <- alm(doi=doi, total_details=TRUE)
  plos_dat2 <- plos_dat[,c("citeulike_total","connotea_total","scopus_total",
                           "counter_total","pmc_total","facebook_shares",
                           "mendeley_shares","twitter_total","date_modified")]
  plos_dat2$date_modified <- as.Date(plos_dat2$date_modified)
  names(plos_dat2) <- c("citeulike","connotea","scopus","ploscounter","pmc",
                        "facebook","mendeley","twitter","date_modified")
  # altmetric
  alt_dat <- altmetric_data(altmetrics(doi=doi))
  if(is.null(alt_dat)){
    alt_dat2 <- data.frame(999,999,999,999,999,999,999,999,"1970-01-01")
  } else
  {
    alt_dat2 <- alt_dat[,c("cited_by_fbwalls_count","cited_by_posts_count",
                           "cited_by_tweeters_count","cited_by_delicious_count",
                           "mendeley","connotea","citeulike","readers_count","last_updated")]
    alt_dat2$last_updated <- as.Date(as.POSIXct(alt_dat2$last_updated, origin="1970-01-01", tz="GMT"))
  }
  names(alt_dat2) <- c("facebook","cite_by_posts_count","twitter","delicious",
                       "mendeley","connotea","citeulike","readers_count","date_modified")
  # impactstory
#   is_dat <- metrics(doi)
  is_dat <- metrics(doi, addifnot=isaddifnot, sleep=1)
  is_dat <- parse_is(is_dat)
  is_dat$date_modified <- as.Date(is_dat$date_modified)
  names(is_dat) <- c("delicious","mendeley","ploshtml","plosviews","scopus","pmc",
                     "twitter","date_modified")
  is_dat$ploscounter <- is_dat$ploshtml + is_dat$plosviews
  is_dat <- is_dat[,-c(3,4)]
  
  # plum
  ## look up plumID, as DOIs can't be used to seearch for plum data via their API 
  plumids_dat <- read.csv("~/github/sac/isqaltms/plum_analytics.csv")[,c(1,3)]
  plum_id <- plumids_dat[grep(doi, as.character(plumids_dat$DOI)), 1]
  ## get data
  plum_dat <- plum(as.character(plum_id))
  
  # put it together
  df <- data.frame(provider=c('PLoS ALM','Altmetric','ImpactStory','PlumAnalytics'), doi=doi, 
             rbind.fill(plos_dat2, alt_dat2, is_dat, plum_dat[,-1]))[,-c(12:14)]
  if(is.null(fileout)){ df } else
  {
    write.table(df, file=fileout, append=TRUE, row.names=FALSE, col.names=FALSE, sep=",")
  }
}

# Safe version in case of errors
compare_altmet_prov_safe <- plyr::failwith(NULL,compare_altmet_prov)

# # Search for and get DOIs for 500 papers
# res <- searchplos(terms="*:*", fields='id', toquery=list('publication_date:[2011-06-30T00:00:00Z TO 2013-01-01T23:59:59Z] ', 'doc_type:full'), start=0, limit=900) 
# write.csv(data.frame(doi = res[1:500,]), "~/github/sac/isqaltms/dois.csv", row.names=FALSE)

# res <- read.csv("~/github/sac/isqaltms/dois.csv")

# # Get the metrics, and make a data.frame
# mydf <- data.frame(NA,NA,999,999,999,999,999,999,999,999,"1970-01-01")
# names(mydf) <- c("provider","doi","citeulike","connotea","scopus","ploscounter","pmc","facebook","mendeley","twitter","date_modified")
# write.csv(mydf, "~/github/sac/isqaltms/alldatout_3.csv", row.names=FALSE)

# alldat <- l_ply(as.character(res[,1])[133:900], compare_altmet_prov_safe, isaddifnot=TRUE, sleep=0, fileout="~/github/sac/isqaltms/alldatout_3.csv", .progress="text")
@

% \begin{figure}[!ht]
%   \centering
%   \includegraphics[width=0.7\textwidth]{figure/dataconst_plot.pdf}
%   \caption{Distribution of absolute differences in least and greatest value of each of seven different altmetrics on a set of 308 DOIs from Altmetric, ImpactStory, and PLoS ALM. Values were log10 transformed to improve comprehension.} % caption line
% \end{figure}

<<dataconst_plot1, eval=TRUE, echo=FALSE, dev=c('png','pdf'), warning=FALSE, fig.cap="Distribution of absolute differences in least and greatest value of each of seven different altmetrics on a set of 565 DOIs from Altmetric, ImpactStory, and PLoS ALM. Values were log10 transformed to improve visual comprehension. Metrics: citeulike = number of Citeulike bookmarks; scopus = number of citations; ploscounter = number of pdf views + html views; pmc = number of Pubmed Central full text + pdf views; facebook = number of Facebook shares; mendeley = number of Mendeley readers; twitter = number of tweets mentioning article.">>=
library(ggplot2, quietly=TRUE)
library(reshape2, quietly=TRUE)
library(reshape, quietly=TRUE)
library(lubridate, quietly=TRUE)

dat <- read.csv("~/github/sac/isqaltms/alldatout_3.csv")[-1,]
dat2 <- read.csv("~/github/sac/isqaltms/plumout.csv")[-1,]
dat2 <- dat2[dat2$doi %in% dat$doi, ]
dat2 <- data.frame(provider='plum', dat2)

alldat <- rbind.fill(dat, dat2)
alldat <- sort_df(alldat, "doi")

dat_split <- split(alldat, f=alldat$doi)
dat_split <- dat_split[!sapply(dat_split, nrow)==0]
dat_split <- compact(lapply(dat_split, function(x) {if(x$citeulike[2]==999){NULL} else(x) }))
dat_split_df <- ldply(dat_split)[,-1]

calcdiff <- function(x){max(na.omit(x))-min(na.omit(x))}

dat_split_df_1 <- ddply(dat_split_df, .(doi), numcolwise(calcdiff))
dat_split_df_melt <- melt(dat_split_df_1)
dat_split_df_melt <- dat_split_df_melt[! dat_split_df_melt$variable %in% "connotea", ] # connotea isn't shared among the providers

ggplot(dat_split_df_melt, aes(x=log10(value), fill=variable)) +
  theme_bw(base_size=14) +
  geom_bar() +
  scale_fill_discrete(name="Almetric") + 
  facet_grid(variable~., scales="free") +
  labs(x=expression(log[10](Value)), y="Count") +
  theme(strip.text.y = element_text(angle=0),
        panel.grid.major = element_blank(),
        panel.grid.minor = element_blank(),
        legend.position = "none",
        panel.border = element_rect(size=1))
# ggsave("~/github/sac/isqaltms/reviews/fig1.tiff", width=8, height=8, dpi=400)
@

What are some possible reasons why similar metrics differ across providers? Numbers could differ for a number of reasons. First, data could be collected from different middle-men. For example, Twitter data is notorious for not being persistent. Thus, you either have to query the Twitter firehose constantly and store data, or go through a company like Topsy (they collect twitter data and charge customers for access; http://topsy.com/) to collect tweets. Whereas ImpactStory collects tweets from Topsy, PLoS ALM and Altmetric collect tweets with an unknown method. Different intermediary data sources could lead to different data. Second, data could be collected at different times. This could easily result in different data even if data are collected from the same source. This is especially obvious as ImpactStory collects some metrics via the PLoS ALM API, so those metrics that ImpactStory has from the PLoS ALM API should be the same as those that PLoS has. Fortunately, date is supplied in the returned data by three of the providers (PLoS ALM, ImpactStory, and Altmetric). Thus, I examined whether or not date could explain differences in metrics from the different providers. Figure 2 shows that there are definitely some large differences in values that could be due differences in date the data was collected, but this is not always the case (i.e., there are a lot of large difference values with very small difference in dates). 

<<dataconst_plot2, eval=TRUE, echo=FALSE, dev=c('png','pdf'), warning=FALSE, fig.cap="Distribution of absolute differences in least and greatest value of each of seven different altmetrics on a set of 565 DOIs from Altmetric, ImpactStory, and PLoS ALM. Values were log10 transformed to improve visual comprehension. See Fig. 1 for explanation of almetrics.">>=
datediff <- function(x){
  datesorted <- sort(x)
  round(as.numeric(difftime(datesorted[3], datesorted[1])),0)
}

dat_split_df2 <- dat_split_df[!dat_split_df$provider %in% "plum",] # remove plum as no updated dates given
dat_split_df_1 <- ddply(dat_split_df2, .(doi), numcolwise(calcdiff))
dat_split_df_2 <- ddply(dat_split_df2, .(doi), summarise, datediff = datediff(date_modified))
dat_split_df_melt <- melt(dat_split_df_1)
dat_split_df_ <- merge(dat_split_df_melt, dat_split_df_2, by="doi")
dat_split_df_melt <- dat_split_df_[! dat_split_df_$variable %in% "connotea", ] # connotea isn't shared among the providers

ggplot(dat_split_df_melt, aes(x=datediff, y=log10(value+1), colour=variable)) +
  theme_bw(base_size=14) +
  geom_point(size=3, alpha=0.6) +
  scale_colour_brewer("Source", type="qual", palette=3) +
  theme(panel.grid.major = element_blank(),
        panel.grid.minor = element_blank(),
        legend.position = c(0.65, 0.9), 
        panel.border = element_rect(size=1),
        legend.key=element_blank(), 
        panel.background=element_rect(colour="white")) +
  guides(col=guide_legend(nrow=2, override.aes=list(size=4))) +
  labs(x="\nDate difference (no. of days)", y="Value of difference between max and min\n")
# ggsave("~/github/sac/isqaltms/reviews/fig2.tiff", width=8, height=8, dpi=400)
@

That was a rough overview of hundreds of DOIs. What do the differences among providers look like in more detail? I used a set of 20 DOIs from the 565 above to show the value of each altmetric from each of the altmetrics providers for each of the 20 DOIs (see Fig. 3). Note that in some cases there is very close overlap in values for the same altmetric on the same DOI across providers, but in some cases the values are very different. 

<<dataconst2, eval=TRUE, echo=FALSE, dev=c('png','pdf'), warning=FALSE, fig.cap="A comparison of seven different altmetrics on a set of 20 DOIs from Altmetric, ImpactStory, and PLoS. This demonstrates how altmetrics can be very similar across providers for some DOIs, but very dissimilar for others. See Fig. 1 for explanation of almetrics.">>=
# Melt the data.frame 
alldat_m <- melt(dat_split_df[1:60,], id.vars=c(1,2,11))

# Remove connotea as it was removed in above chunk
alldat_m <- alldat_m[!alldat_m$variable %in% "connotea",]

# Create a plot
ggplot(na.omit(alldat_m[,-3]), aes(x=doi, y=value, colour=provider)) +
  theme_bw(base_size = 14) + 
  geom_point(size=4, alpha=0.4, position=position_jitter(width=0.15)) +
  scale_colour_manual(values = c('#FC1D00','#FD8A00','#0D71A5','#2CCC00')) +
  facet_grid(variable~., scales='free') +
  theme(axis.text.x=element_blank(),
        strip.text.y=element_text(angle = 0),
        legend.position="top", 
        legend.key = element_blank(),
        panel.border = element_rect(size=1),
        panel.grid.major = element_blank(),
        panel.grid.minor = element_blank()) +
  guides(col = guide_legend(title="", override.aes=list(size=5), 
                            nrow = 1, byrow = TRUE))
ggsave("~/github/sac/isqaltms/reviews/fig3.tiff", width=8, height=8, dpi=400)
@

An example giving what results look like may be instructive. Here is an example of calling the API of each the four providers to combine data from different sources, for the DOI \emph{10.1371/journal.pbio.1001118} \cite{arslan2011} (Table A2). There are many metrics that have exactly the same values among providers, though there are differences, which could be explained by the difference in the date data was collected. For example, PLoS ALM gives 3860 for combined PLoS views, while ImpactStory gives 3746 views. This is undoubtedly explained by the fact that PLoS ALM data was last updated on May 31, 2013, while ImpactStory's data was last updated on May 18, 2013. There are some oddities, however. For example, Altmetric gives nine tweets, while ImpactStory and Plum Analytics only give three tweets, whle PLoS ALM gives zero. ImpactStory's data was updated more recently (May 18, 2013) than that of Altmetric (July 28, 2012), which suggests something different about the way tweets among the two providers are collected as updated date alone can not explain the difference. In fact, Table A1 shows that ImpactStory collects tweets from Topsy (\url{http://topsy.com/}), while Altmetric collects them with an unknown method, which leads to different data. Meanwhile, PLoS ALM collects tweet data from the Twitter API with an internal Java application. 

<<callacrossprov, eval=TRUE, echo=FALSE, results='asis'>>=
library(xtable, quietly=TRUE) 
# out <- compare_altmet_prov(doi='10.1371/journal.pone.0018657')
out <- compare_altmet_prov(doi='10.1371/journal.pbio.1001118')
out$date_modified <- as.character(out$date_modified)
mytable <- xtable(out[,-c(2,4)], align = "ll|cccccccc", caption="Example of combining results across three data providers on one DOI. Note that dates that data were last modified are the same for PLoS ALM and Altmetric, but different for ImpactStory. Missing values represent data that is not given by that provider. See Fig. 1 for explanation of almetrics.", digits=0)
print(mytable, include.rownames = FALSE, caption.placement = 'top')
@

% \begin{figure}[!ht]
%   \centering
%   \includegraphics[width=0.7\textwidth]{figure/dataconst2.pdf}
%   \caption{A comparison of seven different altmetrics on a set of 20 DOIs from Altmetric, ImpactStory, and PLoS.} % caption line
% \end{figure}

The above findings on data consistency in fact suggest that altmetrics are inconsistent among providers of aggregate altmetrics. Casual users, and especially those conducting altmetrics research, should use caution when using altmetrics data from different providers.  

\subsection*{A crosswalk among providers}

As discussed above, when similar data sources are collected by altmetrics providers, ideally, there would be a way to go between, for example, data from Twitter for PLoS, ImpactStory, Altmetric, and PlumAnalytics. Each of the four providers of course has the right to collect metrics as needed for their purposes, but as altmetrics consumers, we should be able to compare data from the same source across providers. In Appendix Table A1, I provide a table to crosswalk metrics for the same data source among providers.

\section*{Altmetrics data provenance}

Altmetrics data comes from somewhere - tweets from Twitter, citations from Web of Science or Scopus, bookmarks from Citeulike, etc. Provenance is concerned with the origin of an object, the ability to trace where an object comes from in case there is any need to check or validate data. 

Why should we care about provenance in altmetrics?  First, in any research field we should prioritize research being verifiable. For this we need the underlying data. Second, in general, altmetrics are based on completely digital data. This means that all use of, research on, hiring decisions based on, and conclusions drawn from, altmetrics data should theoretically be traceable back to the original production of that data. This is somewhat unusual - most research fields are based on data collected at some point that can not be traced - but this should be possible in altmetrics. Second, a specific example will demonstrate the power of data provenance in altmetrics. Imagine if a research paper makes controversial claims using altmetrics data on a set of objects (e.g., scholarly papers). An independent researcher could theoretically drill down into the data collected for that paper, and gain further insight, and potentially dispute or add to the latter mentioned paper.

Data for the same altmetrics resource could be calculated in different ways and collected at different times for the same object. The four providers already provide the date the metrics were updated. However, there is little information available, via their APIs at least, regarding how data were collected, and what, if any, calculations were done on the data before providing the data. The altmetrics community overall would benefit from transparency in how data are collected. 

There are two types of ways to track provenance, via URLs and identifiers. 

ImpactStory provides a field named \emph{provenance\_url} with each metric data source. For example, for a recent paper \cite{piwowar2007}, a GET call to the ImpactStory API returns many metrics, one of which is 10 bookmarks on Delicious. Importantly, they also return the field \emph{provenance\_url}, in this case \url{http://www.delicious.com/url/9df9c6e819aa21a0e81ff8c6f4a52029}, which takes you directly to the human readble page on Delicous from where the data was collected. This is important for researchers as ideally all of our research is replicable. A nice bit about digital data such as altmetrics is that we can trace back final altmetrics from providers such as ImpactStory to their original source.

The PLoS ALM API provides something less obvious with respect to provenance, a field called \emph{events\_url}, which for the same paper above \cite{piwowar2007} returns 82 bookmarks on Citeulike, and the human readable link to where the data was collected \url{http://www.citeulike.org/doi/10.1371/journal.pone.0000308}. 

Plum Analytics does something interesting with respect to provencance - in addition to the cononical URL, they collect alias URLs for each object that they collect metrics on. For example, for the DOI \emph{10.1371/journal.pone.0018657}, they collect many URLs that point to that paper. This makes sense as a digital product is inevitably going to end up living at more than one URL (the internete is a giant copying machine after all), so collecting URL aliases is a good step forward for altmetrics. It appears ImpactStory does this as well.

An important issue with respect to provenance is that data sources sometimes do not give URLs. For example, CrossRef and Facebook don't provide a URL associated with a metric on an object. Therefore, there is no way to go to a URL and verify the data that was given to you by an altmetrics provider. 

All four providers collect multiple identifiers, including DOI, Pubmed ID, PubMed Central ID, and Mendeley UUID. These identifiers are not URLs but can be used to track down an object of interest in the respective database/service where the identifier was created (e.g., a DOI can be used to search for the object using CrossRef here \url{http://crossref.org/}). 

What is ideal with respect to data provenance? Is the link to where the original data was collected enough? Probably so, if no calculations were done on the original data before reaching users. However, some of the providers do give numbers which have been calculated. For example, ImpactStory puts some metrics into context by calculating a percentage relative to a reference set. Ideally, how this is done should be very clear, and verifiable. 

\section*{Putting altmetrics in context}

Raw altmetrics data can be number of tweets, or number of html views on a publishers website. What do these numbers mean? How does the paper or dataset I care about compare to others? ImpactStory gives context to their scores by classifying scores along two dimensions: audience (scholars or public) and type of engagement (view, discuss, save, cite, recommend). Users can then determine whether a product (paper, dataset, etc.) was highly viewed, discussed, saved, cited, or recommended, and by scientists, or by the public. This abstracts away many details; however, users can drill down to the underlying data via their API and web interface.  Altmetric uses a different approach. They provide context for only one metric, the altmetric score. This is a single aggregate metric, the calculation of which is not known. They do provide context for the altmetric score, including how it compares to a) all articles in the same journal, b) all articles in the same journal published within three weeks of the article, c) all articles in the Almetric database, and d) all articles in the Almetric database published within three weeks of the article. Altmetric gives detailed context for some altmetrics, including Facebook, Twitter, and blogs (see e.g., \url{http://altmetric.com/details.php?citation_id=1270911}). PlumAnalytics do not combine altmetrics into a single score as does Almetric, but do bin similar types of altmetrics into captures, citations, social media, mentions, and usage (\url{http://www.plumanalytics.com/metrics.html}; though you can dive into the indivdiual altmetrics). 

One of the advantages of altmetrics is the fact that they measure many different things, important to different stakeholders (public, scholars, funders). Thus, combining altmetrics into a single score defeats one of the advantages of altmetrics over the traditional measure of impact, journal impact factor, a single metric summarising data on citations. The single Altmetric score is at first appealing given its apparent simplicity. However, if altmetrics are to avoid the mistakes befallen on the Journal Impact Factor \cite{ploseditorial}, we should strive for meaningful altmetrics important to different stakeholders, that retain their context (Tweets vs. citations). 

A specific example highlights the importance of context. A recent paper of much interest titled \emph{Glass shape influences consumption rate for alcoholic beverages} \cite{attwood2012} on the date of writing has an Altmetric score of ~316. This score is compared relative to the same journal (PLoS One), and all journals at different points in time. Other altmetrics are reported but are not given any context. ImpactStory reports no single score, gives raw altmetrics data and gives context. For example, ImpactStory reports that there are 149 tweets that mentioned the paper, and this number of tweets puts the paper 97th-100th percentile of all Web of Science indexed articles that year (2012). This context for tweets about an article is more informative than knowing that the paper has an Altmetric score of 316 - people should know the context for what audience tweets represent, and number of tweets relative to a reference set gives a bit of information on the impact of the paper relative to others. Of course not all journals are indexed by Web of Science, and the important reference set for one person (e.g., papers in journals in their specific field) may be different from another (e.g., papers for colleagues at their university or department). PLoS recently started reporting "Relative Metrics" in the html versions of their articles, where once can compare article usage (cumulative views) to references sets of articles in different fields \cite{allen2013}.

There is still work to do with respect to context. Future work should consider further dimensions of context. For example, perhaps users should be able to decide how to put their metrics into context - instead of getting raw values and values relative to a pre-chosen reference set, users could choose what reference they want to use for their specific purpose. In addition, but much harder to achieve, is sentiment, or the meaning of the mention. That is, was a tweet or citation about a paper mentioned in a negative or positive light?

\section*{Historical context}

Researchers asking questions about altmetrics could ask more questions specifically dealing with time if historical altmetrics data were available. PLoS provides historical altmetrics data on some of their metrics (except in case of licenses, e.g., Web of Science, Scopus), while Altmetric provides only historical data on their Altmetric score (see below section \emph{Putting altmetrics in context}), and ImpactStory and Plum Analytics do not provide historical data. The data returned, for example, for number of tweets for an article from ImpactStory, Altmetric, or Plum Analytics is a cumulative sum of the tweets mentioning that article. What were the number of tweets mentioning the article one month ago, six months ago, one year ago? It is a great feature of PLoS ALM that you can get historical altmetrics data - in fact, PLoS wants this data themselves for things like pattern detection and anti-gaming, so providing the data to users is probably not much additional work. However, these historical data are only available for PLoS articles. The altmetrics community would benefit greatly from storing and making available historical almetrics data. Historical data, especially as more products are tracked, will become expensive to store, so perhaps won't be emphasized by altmetrics providers. In addition, a technical barrier comes in to play in that pushing a lot of data via an API call can get very time consuming. 

% \section*{Relavance of altmetrics}
% The internet has facilitated the existence of altmetrics, as measures of impact are all around us, and many are now machine readable. However, which altmetrics are % relavant? More importantly, which altmetrics are relavant to the community you care about? XXXX. [NOT SURE IT MAKES SENSE TO KEEP THIS SECTION???]

%\section*{Open altmetrics}

%The Journal Impact Factor (JIF), based on citation counts, the predecessor in a way to altmetrics, is a closed metric, calculated by one company, the calculation of which is unknown \cite{rossner2007show}. Given that the altmetrics movement is just at its begining, it has the chance to be a more transparent endeavour. There are a number of companies and apps appearing to provide services on top of altmetrics, some of which to create profits need to protect data outputs and the details of their operations.  Regardless, I think the altmetrics community can and should provide a level of transparency and openness that will allow research on, and use of, altmetrics. Openness in altmetrics can only be good for the science. If altmetrics are transparent, conclusions from papers, hiring decisions, and more can be verified by third parties. In addition, transparent altmetrics are less likely to be gamed, as they JIF has been gamed \cite{arnold2011}. Some good steps have been made to open altmetrics by altmetrics providers themselves. For example, many conferences and meetings have occurred where altmetrics providers and other interested parties hash out issues. 

%Another issue to consider in open altmetrics are the primary altmetrics sources. The difference between Twitter and App.net is a good example. Twitter makes money off advertisements and having a relatively closed system (\url{http://techonomy.com/2012/08/video-jack-dorsey-on-twitters-business-model/}). A better option, with similar data, would be App.net. App.net has a much smaller set of users relative to Twitter - however, App.net's business model is to charge users to use the service, but make the data very open. Twitter makes more money from closing down data, whereas App.net makes more money from opening up their data. There is currently a heavy dependence on Twitter in the altmetrics community. Perhaps we should think about building the scientific community on App.net where openness can thrive. 

\section*{Technical barriers to use}

Some altmetrics users may only require basic uses of altmetrics, like including altmetrics on their CV's \cite{piwowar2013power} to show the various impacts of their research. Some may want to go deeper, and perhaps collect altmetrics at finer time scales, or with more detailed data, than are given by altmetrics aggregate providers. What are the barriers to getting more detailed altmetrics data? 

Diving deeper into altmetrics means considering whether one can access data, whether the data source is machine readable, and how easy the data is to retrieve and manipulate once retrieved. 

\begin{itemize}
  \item \emph{Data access} Many altmetrics sources are accessible as the data providers have open, or at least partly open, APIs (e.g., Crossref, PLoS). Other data sources can provide problems. For example, you can only get tweets from Twitter for the past 30 days, after which point you have to pay for a service that caches historical Twitter data (e.g, Topsy). Others are totally inaccessable (e.g., Google Scholar citations). 
  \item \emph{Machine readable} Ideally, altmetrics are provided through an API. However, some metrics of interest may only be in PDFs, spreadsheets, or html, which can not be easily consumed and mashed up. For these metrics, the user should seek out aggregators such as those discussed in this paper to do the heavy lifting. Alternatively, technically savy researchers could write their own code, or leverage tools such as ScraperWiki \url{https://scraperwiki.com/}. 
  \item \emph{Ease of use} Fortunately, many libraries, or extensions, exist for a number of programming languages relavant to scholars (Python, R), which deal with interacting with altmetricsc data (e.g., Figshare API libraries \cite{figshare}, Twitter API libraries \cite{twitter}). See Table 1 for links to libraries for aggregate providers. These libraries take care of the data collection and transform data to user friendly objects, allowing users to do the real work of analysis and inference.
\end{itemize}

%Last, authentication can be a barrier to use in that if a user has to take extra step of authenticating, they may not bother. There are a variety of possible authentication methods, some of which include: a) no authentication, b) username and password pair, c) API key, and d) OAuth (including OAuth1 and OAuth2) (Table 1). These different options make sense in different use cases. The first, no authentication, used by PLoS, makes sense when an API is first released and testers are needed to get feedback. A benefit of an API with no authentication is the barrier to entry is lower. That is, if you don't have to ask a user to register to get an API key they are more likely to use the API. The second and third options, username/password pair and API key are relatively similar; API keys are used by both ImpactStory and Altmetric (Table 1). The last option, OAuth, is not used by any of the altmetrics providers. This authentication method is however used by many API providers. From the viewpoint of a consumer in a desktop scripting language, OAuth can be painful. What works better for scripting languages are the first three options.  

\section*{Conclusion}

Altmetrics measure the impact of scholarly articles and other products (e.g., datasets, presentations). These measures of scholarly impact are quickly gaining ground as evidenced by the four companies aggregating and providing altmetrics (see Table 1). In any field growing pains are inevitable - altmetrics as a field is quite young, and therefore has some issues to work out. I have shown in this paper that while the four providers aren't doing anything wrong or intentionally misleading, altmetrics users should think about a variety of issues when using altmetrics data: consistency, provenance, and context. Altmetrics providers collect data at different times, and from different sources; combining data across providers should be done with care. Altmetrics is special in the sense that all data is digital. Thus, there is no reason we shouldn't be able to track all altmetrics data to their sources. This will not only provide additional insight to scholarly impact, but provide a way to verify results and conclusions made regarding altmetrics. 

As altmetrics grow in use and popularity, researchers will ask more questions about altmetrics. In addition, it is hard to predict what people will want to do with altmetrics data in the future. Since we are in the early stages of the field of altmetrics, we have the chance to steer the altmetrics ship in the right direction. I hope the points covered in this paper provide fodder almetrics providers and users to consider.

\section*{Acknowledgments}

I thank Martin Fenner for inviting me to write a paper in this special issue on altmetrics, and for extremely helpful feedback from Carl Boettiger and Karthik Ram on earlier versions of this manuscript.
  
\section*{References}
\bibliography{refs}

\newpage

\section*{Appendix A. Crosswalk among providers.}

The following Table A1 provides a crosswalk between altmetrics data collected by the three data providers. Note that these variables relate to one another across providers, but the data may be collected differently, and so for example, altmetrics collected for Twitter may differ between PLoS, ImpactStory and Altmetric. Where data sources are shared among at least two providers, I used only those fields that would give the same data if data were collected on the same date and all other things being equal. For example, PLoS ALM's field \emph{pubmed} is equivalent to ImpactStory's \emph{pubmed:pmc\_citations} field.

\begin{table}[!ht]
\begin{threeparttable}[b]
\caption{Data sources used in taxize, tasks available, and links to them}
\begin{tabular}[t]{|B|B|B|B|D}
\hline
\headcol Data source & PLoS\tnote{a} & ImpactStory\tnote{b} & Altmetric\tnote{c} & PlumAnalytics\tnote{d} \\
\hline
Biod & biod & No & No & No \\
Bloglines & bloglines & No & No & No \\
Nature blogs & nature & No & No & No\\
Researchblogging & researchblogging & No & No & ResearchBlogging \\
WebOfScience citations & webofscience & No & No & No \\
Dryad & No & dryad:total\_downloads package\_views & No & Views, downloads \\
Figshare & No & figshare:views shares downloads & No & Recommendations, downloads, views \\
Github & No & github:forks stars & No & Collaborators, downloads, followers, forks, watches, gists \\
PLoS Search & No & plossearch:mentions & No & No \\
Slideshare & No & slideshare:favorites views comments downloads & No & Downloads, favorites, comments \\
Google+ & No & No & cited by gplus count & No. +1's \\
MSM & No & No & cited by msm count & No \\
News articles & No & No & Yes & Yes \\
Reddit & No & No & cited by rdts count & Comments, upvotes-downvotes \\
Citeulike & citeulike & citeulike:bookmarks & No & Citeulike \\
Crossref & crossref & plosalm:crossref\tnote{e} & No & No \\
PLoS ALM & counter(pdf\_views + html\_views)\tnote{f} & plosalm(html\_views, pdf\_views) & No & Views of abstract, figures, full text, html, pdf, supporting data \\
PMC & pmc & plosalm:pmc\_full-text + pmc\_pdf\tnote{g} & No & No \\
PubMed & pubmed & pubmed:pmc\_citations\tnote{h} & No & Pumbed \\
Scienceseeker & scienceseeker & scienceseeker:blog\_posts & No & ScienceSeeker \\
Scopus citations & scopus & plosalm:scopus\tnote{i} & No & Scopus \\
Wikipedia & wikipedia & wikipedia:mentions & No & Wikipedia \\
Delicious & No & delicious:bookmarks & cited by delicious count & Delicious \\
Facebook & facebook\_shares & facebook:shares\tnote{j} & cited by fbwalls count & Facebook clicks, comments, likes \\
Mendeley readers & mendeley shares & mendeley readers\tnote{k} & mendeley readers & Mendeley readers, groups \\
Twitter & twitter & topsy:tweets\tnote{l} & cited by tweeters count & Topsy tweets \\
\hline
\end{tabular}
\end{threeparttable}
\end{table}
% \begin{tablenotes}
%     \item[a] These are the exact names for each data source in the PLos ALM API. For example: \url{http://alm.plos.org/api/v3/articles?ids=10.1371/journal.pone.0018657&source=twitter}.
%     \item[b] You can not request a specific source from the ImpactStory API, so these are the names of the fields in the returned json. For example, see the json from this call: \url{http://api.impactstory.org/v1/item/doi/10.1371/journal.pone.0018657?key=YOURAPIKEY}.
%     \item[c] You can not request a specific source from the Altmetric API, so these are the names of the fields in the returned json. For example, see the json from this call: \url{http://api.altmetric.com/v1/doi/10.1371/journal.pbio.0018657?key=YOURAPIKEY}.
%     \item[d] Some of these names are the exact names returned in an API call; others are not.
%     \item[e] Collected from the PLoS ALM API. 
%     \item[f] PLoS ALM also provides xml\_views. 
%     \item[g] Collected from the PLoS ALM API. Other PMC data fields collected from PLoS ALM (pmc\_abstract, pmc\_supp-data, pmc\_figure, pmc\_unique-ip) and from PubMed (suppdata\_views, figure\_views, unique\_ip\_views, pdf\_downloads, abstract\_views, fulltext\_views).
%     \item[h] Should be equivalent to plosalm:pubmed\_central. ImpactStory also collects pubmed:pmc\_citations\_reviews f1000 pmc\_citations\_editorials.
%     \item[i] Collected from the PLoS ALM API. Scopus citations also collected from Scopus itself, in the field scopus:citations.
%     \item[j] ImpactStory also collects Facebook clicks, comments, and likes. 
%     \item[k] ImpactStory also collects Mendeley readers by discipline, number of groups that have added the article, percent of readers by country, and percent of readers by career\_stage. 
%     \item[l] ImpactStory also collects the number of influential\_tweets from Topsy.
% \end{tablenotes}


\newpage

Footnotes from Table A1.
\singlespacing
\begin{itemize}
    \item[a] These are the exact names for each data source in the PLos ALM API.
    \item[b] You can not request a specific source from the ImpactStory API, so these are the names of the fields in the returned JSON from a call.
    \item[c] You can not request a specific source from the Altmetric API, so these are the names of the fields in the returned JSON from a call.
    \item[d] Some of these names are the exact names returned in an API call; others are not.
    \item[e] Collected from the PLoS ALM API. 
    \item[f] PLoS ALM also provides xml\_views. 
    \item[g] Collected from the PLoS ALM API. Other PMC data fields collected from PLoS ALM (pmc\_abstract, pmc\_supp-data, pmc\_figure, pmc\_unique-ip) and from PubMed (suppdata\_views, figure\_views, unique\_ip\_views, pdf\_downloads, abstract\_views, fulltext\_views).
    \item[h] Should be equivalent to plosalm:pubmed\_central. ImpactStory also collects pubmed:pmc\_citations\_reviews f1000 pmc\_citations\_editorials.
    \item[i] Collected from the PLoS ALM API. Scopus citations also collected from Scopus itself, in the field scopus:citations.
    \item[j] ImpactStory also collects Facebook clicks, comments, and likes. 
    \item[k] ImpactStory also collects Mendeley readers by discipline, number of groups that have added the article, percent of readers by country, and percent of readers by career\_stage. 
    \item[l] ImpactStory also collects the number of influential\_tweets from Topsy.
\end{itemize}

\end{document}