\documentclass[letterpaper,superscriptaddress,showkeys,longbibliography]{revtex4-1}
\usepackage[utf8]{inputenc}
\usepackage{color,dcolumn,graphicx,hyperref}
\usepackage{natbib}
\usepackage{footnote}
\usepackage{scrextend}

% stuff for coloring rows in tables - BEGIN
\usepackage{booktabs}% http://ctan.org/pkg/booktabs
\usepackage{colortbl}% http://ctan.org/pkg/colortbl
\usepackage{amsmath}% http://ctan.org/pkg/amsmath
\usepackage{xcolor}% http://ctan.org/pkg/xcolor
\usepackage{graphicx}% http://ctan.org/pkg/graphicx

\colorlet{tableheadcolor}{gray!25} % Table header colour = 25% gray
\newcommand{\headcol}{\rowcolor{tableheadcolor}} %
\colorlet{tablerowcolor}{gray!10} % Table row separator colour = 10% gray
\newcommand{\rowcol}{\rowcolor{tablerowcolor}} %
    % Command \topline consists of a (slightly modified) \toprule followed by a \heavyrule rule of colour tableheadcolor (hence, 2 separate rules)
\newcommand{\topline}{\arrayrulecolor{black}\specialrule{0.1em}{\abovetopsep}{0pt}%
            \arrayrulecolor{tableheadcolor}\specialrule{\belowrulesep}{0pt}{0pt}%
            \arrayrulecolor{black}}
    % Command \midline consists of 3 rules (top colour tableheadcolor, middle colour black, bottom colour white)
\newcommand{\midline}{\arrayrulecolor{tableheadcolor}\specialrule{\aboverulesep}{0pt}{0pt}%
            \arrayrulecolor{black}\specialrule{\lightrulewidth}{0pt}{0pt}%
            \arrayrulecolor{white}\specialrule{\belowrulesep}{0pt}{0pt}%
            \arrayrulecolor{black}}
    % Command \rowmidlinecw consists of 3 rules (top colour tablerowcolor, middle colour black, bottom colour white)
\newcommand{\rowmidlinecw}{\arrayrulecolor{tablerowcolor}\specialrule{\aboverulesep}{0pt}{0pt}%
            \arrayrulecolor{black}\specialrule{\lightrulewidth}{0pt}{0pt}%
            \arrayrulecolor{white}\specialrule{\belowrulesep}{0pt}{0pt}%
            \arrayrulecolor{black}}
    % Command \rowmidlinewc consists of 3 rules (top colour white, middle colour black, bottom colour tablerowcolor)
\newcommand{\rowmidlinewc}{\arrayrulecolor{white}\specialrule{\aboverulesep}{0pt}{0pt}%
            \arrayrulecolor{black}\specialrule{\lightrulewidth}{0pt}{0pt}%
            \arrayrulecolor{tablerowcolor}\specialrule{\belowrulesep}{0pt}{0pt}%
            \arrayrulecolor{black}}
    % Command \rowmidlinew consists of 1 white rule
\newcommand{\rowmidlinew}{\arrayrulecolor{white}\specialrule{\aboverulesep}{0pt}{0pt}%
            \arrayrulecolor{black}}
    % Command \rowmidlinec consists of 1 tablerowcolor rule
\newcommand{\rowmidlinec}{\arrayrulecolor{tablerowcolor}\specialrule{\aboverulesep}{0pt}{0pt}%
            \arrayrulecolor{black}}
    % Command \bottomline consists of 2 rules (top colour
\newcommand{\bottomline}{\arrayrulecolor{white}\specialrule{\aboverulesep}{0pt}{0pt}%
            \arrayrulecolor{black}\specialrule{\heavyrulewidth}{0pt}{\belowbottomsep}}%
\newcommand{\bottomlinec}{\arrayrulecolor{tablerowcolor}\specialrule{\aboverulesep}{0pt}{0pt}%
            \arrayrulecolor{black}\specialrule{\heavyrulewidth}{0pt}{\belowbottomsep}}%
% stuff for coloring rows in tables - BEGIN

\newcolumntype{L}{>{\arraybackslash}m{5cm}} % creates now column type to wrap text
\newcolumntype{B}{>{\arraybackslash}m{3cm}} % creates now column type to wrap text
\newcolumntype{C}{>{\arraybackslash}m{3.5cm}} % creates now column type to wrap text

\hypersetup
{
    colorlinks = true, linkcolor = blue, citecolor = blue, urlcolor = blue,
}

\begin{document}

<<setup, include=FALSE, cache=FALSE>>=
## set global chunk options
opts_chunk$set(fig.path='figure/', fig.align='center', fig.show='hold', par=TRUE, warning=FALSE, message=FALSE, error=FALSE, comment=NA)

knit_hooks$set(plot = function(x, options) {
  paste('<figure><img src="',
        opts_knit$get('base.url'), paste(x, collapse = '.'),
        '"><figcaption>', options$fig.cap, '</figcaption></figure>',
        sep = '')
})
@

\title{Consuming altmetrics: some observations and lessons}

\author{Scott Chamberlain}
\email[E-mail: ]{scott@ropensci.org}
\affiliation{Biology Dept., Simon Fraser University, Burnaby, BC, Canada V5B 1E1}

\keywords{altmetrics; R; sotware; API}

\maketitle

\section{Abstract}

\section{Introduction}

Altmetrics, or article level metrics, measure the impact of individual articles, or objects. This is in stark contrast to the impact factor, which is a proprietary summation of the impact of all articles in a journal. Altmetrics have many advantages over journal level metrics, including quantifying more than just citations, and providing metrics on a variety of impacts (e.g., discussed by the media (mentions in the news), discussed by the public (facebook likes, tweets), and importance to colleagues (citations)). 

Altmetrics can be consumed in a variety of contexts: as static text, images, or graphs alongside a pdf or html page, as a javascript widget in an html page, and more. A use case that will, and should, be increasingly common is using scripting languages (e.g., Python, Ruby, R) to consume altmetrics on a machine locally, often for purposes of research on altmetrics themselves. Consuming altmetrics from this perspective is somewhat different than the typical use case where a user looks at altmetrics in a web browser. 

This paper takes a look at the perspective of developing and using scripting interfaces to altmetrics. From this perspective, there are a number of considerations: data stanardization and consistency; API speed; authentication; and data provenance. First, I'll go over the current altmetrics data providers. 

\section{Altmetrics data providers}

There are many publishers that are now presenting altmetrics alongside their papers on their websites. However, these publishers do not yet provide public facing APIs (Application Programming Interface) at the time of writing. There are four entities that aggreagate and provide altmetrics data, one of which (Plum Analytics) does not have an open public facing API, so will not be discussed further.  Three altmetrics providers, PLoS, ImpactStory, and Altmetric, are similar in some ways, and different in others. PLoS and ImpactStory have open APIs, while Altmetrics API is limited to X API requests per X. PLoS provides data in JSON (JavaScript Object Notation) and XML (Extensible Metadata Language), ImpactStory in JSON only, and Altmetric in JSON and JSONP. In terms of granularity, PLoS provides much more granular data than the others, with daily, monthly and yearly totals; ImpactStory provides only total values; and Altmetric provides total values, plus incremental summaries of their proprietary Altmetric score. PLoS is a publisher, while the mission of the other two is to collect and provide altmetrics data. PLoS and ImpactStory are non-profit, while Altmetric is for-profit.

The three providers overlap in some sources of altmetrics they gather, but not all. The fact that there is some complementarity in sources opens the possibility that metrics can be combined from across the different providers. For those that are compelementary, this should be relatively easy. However, when they share data sources, data may not be consistent between providers for the same data source (see _Data standardization and consistency_ below).


\begin{table}[!ht]
\caption{Details on the current altmetrics providers}\label{tab:a} % title of Table
\begin{tabular}{|l|l|l|l|}
\hline
Variable & PLoS & ImpactStory & Altmetric  \\
\hline
Open API? & Yes & Yes & Limited \\
Data format & JSON,XML & JSON & JSON,JSONP \\
Granularity\footnote{D: day; M: month; Y: year; T: total; I: incremental summaries} & D,M,Y & T & I \\
Authentication & None & API key & API key \\
Business type & Publisher & Altmetrics provider & Altmetrics provider \\
Business model & Non-profit & Non-profit & For-profit \\
\hline
\end{tabular}
\end{table}

\section{Data standardization and consistency}

Now that there are multiple providers for altmetrics data, data consistency is something to keep in mind. For example, PLoS, ImpactStory and Altmetric do collect altmetrics from some of the same data sources. Are the numbers they present to users the same for the same paper, or are they different due to different collection dates or methods of collection? Each of the three providers of course has the right to collect metrics as needed for their purposes, 

I retrieved a set of 100 more or less random DOIs for full articles from PLoS journals - this way all three providers would have data on the papers. I collected metrics from each of the three providers for each of the 100 DOIs. 

<<dataconsistency, eval=FALSE, echo=FALSE, warning=FALSE, fig.cap="Fig. 1. A comparison of eight different altmetrics on a set of 20 DOIs from Altmetric, ImpactStory, and PLoS.">>=
suppressPackageStartupMessages(library(alm, quietly=TRUE))
library(rImpactStory, quietly=TRUE)
library(rAltmetric, quietly=TRUE); library(ggplot2, quietly=TRUE)

# Define a function to parse ImpactStory results
parse_is <- function(input){
  metrics <- input$metrics
  delicious_bookmarks <- try(metrics$`delicious:bookmarks`$values[["raw"]])
  mendeley_readers <- try(metrics$`mendeley:readers`$values[["raw"]])
  plosalm_html_views <- try(metrics$`plosalm:html_views`$values[["raw"]])
  plosalm_pdf_views <- try(metrics$`plosalm:pdf_views`$values[["raw"]])
  plosalm_scopus <- try(metrics$`plosalm:scopus`$values[["raw"]])
  pmc_citations <- try(metrics$`pubmed:pmc_citations`$values[["raw"]])
  topsy_tweets <- try(metrics$`topsy:tweets`$values[["raw"]])
  results <- list(delicious_bookmarks=delicious_bookmarks,mendeley_readers=mendeley_readers,
                  plosalm_html_views=plosalm_html_views,plosalm_pdf_views=plosalm_pdf_views,
                  plosalm_scopus=plosalm_scopus,pmc_citations=pmc_citations,topsy_tweets=topsy_tweets)
  results[sapply(results, is.null)] <- NA
  data.frame(results, date_modified=input$last_modified)
}

# Define a function to collect altmetrics from three providers
compare_altmet_prov <- function(doi){
  
  plos_dat <- alm(doi=doi, total_details=TRUE)
  plos_dat2 <- plos_dat[,c("citeulike_total","connotea_total","scopus_total",
                           "counter_total","pmc_total","facebook_shares",
                           "mendeley_shares","twitter_total","date_modified")]
  plos_dat2$date_modified <- as.Date(plos_dat2$date_modified)
  names(plos_dat2) <- c("citeulike","connotea","scopus","ploscounter","pmc",
                        "facebook","mendeley","twitter","date_modified")
  
  alt_dat <- altmetric_data(altmetrics(doi=doi))
  alt_dat2 <- alt_dat[,c("cited_by_fbwalls_count","cited_by_posts_count",
                         "cited_by_tweeters_count","cited_by_delicious_count",
                         "mendeley","connotea","citeulike","readers_count","last_updated")]
  alt_dat2$last_updated <- as.Date(as.POSIXct(alt_dat2$last_updated, origin="1970-01-01", tz="GMT"))
  names(alt_dat2) <- c("facebook","cite_by_posts_count","twitter","delicious",
                       "mendeley","connotea","citeulike","readers_count","date_modified")
  
  is_dat <- metrics(doi)
  is_dat <- parse_is(is_dat)
  is_dat$date_modified <- as.Date(is_dat$date_modified)
  names(is_dat) <- c("delicious","mendeley","ploshtml","plosviews","scopus","pmc",
                     "twitter","date_modified")
  is_dat$ploscounter <- is_dat$ploshtml + is_dat$plosviews
  is_dat <- is_dat[,-c(3,4)]
  
  data.frame(provider=c('plosalm','altmetric','impactstory'), doi=doi, 
             rbind.fill(plos_dat2, alt_dat2, is_dat))[,-c(12:14)]
  }

# Make a vector of twenty DOIs of PLoS papers
dois <- c('10.1371/journal.pone.0018657','10.1371/journal.pone.0058869',
          '10.1371/journal.pone.0060278','10.1371/journal.pone.0060410',
          '10.1371/journal.pone.0052988','10.1371/journal.pone.0062474',
          '10.1371/journal.pone.0059030','10.1371/journal.pbio.1001127',
          '10.1371/journal.pone.0034911','10.1371/journal.pgen.1002873',
          '10.1371/journal.pone.0021416','10.1371/journal.pone.0026907',
          '10.1371/journal.pone.0050634','10.1371/journal.pone.0001230',
          '10.1371/journal.pone.0017114','10.1371/journal.pone.0045308',
          '10.1371/journal.pone.0034916','10.1371/journal.pone.0045307',
          '10.1371/journal.pone.0043446','10.1371/journal.pone.0024135')

# Get the metrics, and make a data.frame
alldat <- ldply(dois, compare_altmet_prov)

# Melt the data.frame 
alldat_m <- melt(alldat, id.vars=c(1,2,11))

# Create a plot
ggplot(na.omit(alldat_m), aes(x=doi, y=value, colour=provider)) +
  theme_bw(base_size = 16) + 
  geom_point(size=3, alpha=0.5) +
  scale_colour_manual(values = c('#90977E','#E0C99F','#9D7763')) +
  facet_grid(variable~., scales='free') +
  theme(axis.text.x=element_blank(),
        strip.text.y=element_text(angle = 0),
        legend.position="top", 
        legend.key = element_blank()) +
  guides(col = guide_legend(title="", override.aes=list(size=4), 
                            nrow = 1, byrow = TRUE))
@

\subsection*{A crosswalk among providers}

As discussed above, when similar data sources are collected by altmetrics providers, ideally, there would be a way to go between, for example, data from Twitter for PLoS, ImpactStory, and Altmetric. Each of the three providers of course has the right to collect metrics as needed for their purposes, but as altmetrics consumers, we should be able to compare data from the same source across providers. In Appendix Table A1, I provide a table to crosswalk metrics for the same data source among providers.

\section{API Speed}

The speed of API calls is not likely something most users for altmetrics providers will notice. However, developers of applications on top of altmetrics providers will definitely notice differences in API speed. A quick analysis of the speed of API requests shows that PLoS is about twice as fast as Altmetric, and ImpactStory is many times slower than both PLoS and Altmetric. This test was done using a MacBook Pro, mid-2010, 4 core machine, 2.5 GHz Intel processor. 

<<apispeed, eval=FALSE, echo=FALSE, warning=FALSE>>=
library(rbenchmark, quietly=TRUE)

# Run the benchmark tests
benchmark(replications=3,
          PLoS = alm(doi=dois[10], total_details=TRUE),
          Altmetric = altmetrics(doi=dois[10]),
          ImpactStory = metrics(dois[10]),
          order = "elapsed",
          columns=c('test', 'elapsed', 'replications'))
@

Another aspect of API speed is the ability to pass in many queries simultaneously. The PLoS API allows you to pass in many DOIs or other identifiers to retrieve altmetrics, whereas the Altmetric and ImpactStory APIs only allow one identifier per call. Allowing many identifiers per call can dramatically speed up altmetrics harvesting. Another option to speed up calls is parallelizing calls across cores of a machine or many machines.  However, this will cause many calls in a very short time period, likely violating the rules/guidelines issued by the providers.


\section{Authentication}

There are a variety of possible authentication methods, some of which include: a) no authentication, b) username and password pair, c) API key, and d) OAuth (including OAuth1 and OAuth2) (Table 1). These different options make sense in different use cases. The first, no authentication, used by PLoS, makes sense when an API first comes out and testers are needed to get feedback on the API. A benefit of an API with no authentication is the barrier to entry is lower. That is, if you don't have to ask a user to register to get an API key they are more likely to use the API. The second and third options, username/password pair and API key are relatively similar; API keys are used by both ImpactStory and Altmetric (Table 1). The last option, OAuth, is not used by any of the altmetrics providers. This authentication method is however used by many API providers. From the viewpoint of a consumer in a desktop scripting language, OAuth can be painful. What works better for scripting languages are the first three options.  

\section{Data provenance}

Data for the same altmetrics resource could be calculated in different ways and collected at different times for the same object. The three providers already provide the date the metrics were updated. However, there is no information available, via their APIs at least, regarding how data were collected, and what, if any, calculations were done on the data before providing the data. The for-profit providers, Altmetric and Plum Analytics, have no obligation to share these, but the altmetrics community overall would benefit from transparency in how data are collected. 

\section{Conclusion}

XXXXXX

\section{Acknowledgments}

I thank Martin Fenner for inviting me to write a paper in this special issue, and for feedback from X, Y, and Z on earlier versions of this manuscript.

\bibliography{refs}

\section{Appendix A. Crosswalk table among providers.}

The following Table A1 provides a crosswalk between altmetrics data collected by the three data providers. Note that these variables relate to one another across providers, but the data may be collected differently, and so for example, altmetrics collected for Twitter may differ between PLoS, ImpactStory and Altmetric.

\begin{table}[!ht]
\caption{Data sources used in taxize, tasks available, and links to them}
\begin{tabular}{|B|B|L|C|}
\hline
\headcol Data source & PLoS\footnote{These are the exact names for each data source in the PLos ALM API. For example: \url{http://alm.plos.org/api/v3/articles?ids=10.1371/journal.pone.0018657&source=twitter}} & ImpactStory\footnote{You can not request a specific source from the ImpactStory API, so these are the names of the fields in the returned json. For example, see the json from this call: \url{http://api.impactstory.org/v1/item/doi/10.1371/journal.pone.0018657?key=YOURAPIKEY}} & Altmetric\footnote{You can not request a specific source from the Altmetric API, so these are the names of the fields in the returned json. For example, see the json from this call: \url{http://api.altmetric.com/v1/doi/10.1371/journal.pbio.0018657?key=YOURAPIKEY}} \\
\hline
Biod & biod & No & No \\
\rowcol Connotea & connotea & No & No \\
General blogs & bloglines & No & No \\
\rowcol Nature blogs & nature & No & No \\
Postgenomic & postgenomic & No & No \\
\rowcol Researchblogging & researchblogging & No & No \\
WebOfScience citations & webofscience & No & No \\
\rowcol Dryad & No & dryad:total\_downloads package\_views & No \\
Figshare & No & figshare:views shares downloads & No \\
\rowcol Github & No & github:forks stars & No \\
PLoS Search & No & plossearch:mentions & No \\
\rowcol Slideshare & No & slideshare:favorites views comments downloads & No \\
Google+ & No & No & cited\_by\_gplus\_count \\
\rowcol MSM & No & No & cited\_by\_msm\_count \\
News articles & No & No & Yes \\
\rowcol Reddit & No & No & cited\_by\_rdts\_count \\
Citeulike & citeulike & citeulike:bookmarks & No \\
\rowcol Crossref & crossref & plosalm:crossref\footnote{\label{a1}Collected from the PLoS ALM API.} & No \\
PLoS ALM & counter(pdf\_views html\_views xml\_views) & plosalm(html\_views, pdf\_views) & No \\
\rowcol PMC & pmc & pmc(suppdata\_views figure\_views unique\_ip\_views pdf\_downloads abstract\_views fulltext\_views); (plosalm:pmc\_abstract pmc\_supp-data pmc\_figure pmc\_full-text pmc\_pdf pmc\_unique-ip)\footref{a1} & No \\
PubMed & pubmed & pubmed:pmc\_citations\_reviews f1000 pmc\_citations\_editorials pmc\_citations (plosalm:pubmed\_central)\footref{a1} & No \\
\rowcol Scienceseeker & scienceseeker & scienceseeker:blog\_posts & No \\
Scopus citations & scopus & scopus:citations; (plosalm:scopus)\footref{a1} & No \\
\rowcol Wikipedia & wikipedia & wikipedia:mentions & No \\
Delicious & No & delicious:bookmarks & cited\_by\_delicious\_count \\
\rowcol Facebook & facebook & facebook:shares clicks comments likes & cited\_by\_fbwalls\_count \\
Mendeley & mendeley & mendeley:discipline readers groups country career\_stage & mendeley \\
\rowcol Twitter & twitter & topsy:influential\_tweets tweets & cited\_by\_tweeters\_count \\
\hline
\end{tabular}
\end{table}


Here is an example of calling the API of each the three providers to combine data from different sources.

<<callacrossprov, eval=FALSE, echo=FALSE, warning=FALSE>>=
(out <- compare_altmet_prov(doi='10.1371/journal.pone.0018657'))
@

\end{document}